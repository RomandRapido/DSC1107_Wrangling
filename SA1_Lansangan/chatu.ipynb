{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data from: --f=c:\\Users\\riche\\AppData\\Roaming\\jupyter\\runtime\\kernel-v399914cc4074d9c61efb3c5f4e893f0d21250b287.json\n",
      "Error in main execution: [Errno 22] Invalid argument: '--f=c:\\\\Users\\\\riche\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v399914cc4074d9c61efb3c5f4e893f0d21250b287.json'\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Traceback (most recent call last):\n",
      "  File \"D:\\tmp\\ipykernel_6104\\1118306964.py\", line 285, in main\n",
      "    df = load_data(file_path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"D:\\tmp\\ipykernel_6104\\1118306964.py\", line 16, in load_data\n",
      "    df = pd.read_csv(file_path)\n",
      "         ^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1026, in read_csv\n",
      "    return _read(filepath_or_buffer, kwds)\n",
      "           ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 620, in _read\n",
      "    parser = TextFileReader(filepath_or_buffer, **kwds)\n",
      "             ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1620, in __init__\n",
      "    self._engine = self._make_engine(f, self.engine)\n",
      "                   ^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pandas\\io\\parsers\\readers.py\", line 1880, in _make_engine\n",
      "    self.handles = get_handle(\n",
      "                   ^^^^^^^^^^^\n",
      "  File \"d:\\Anaconda\\Lib\\site-packages\\pandas\\io\\common.py\", line 873, in get_handle\n",
      "    handle = open(\n",
      "             ^^^^^\n",
      "OSError: [Errno 22] Invalid argument: '--f=c:\\\\Users\\\\riche\\\\AppData\\\\Roaming\\\\jupyter\\\\runtime\\\\kernel-v399914cc4074d9c61efb3c5f4e893f0d21250b287.json'\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import StandardScaler, OneHotEncoder\n",
    "from sklearn.compose import ColumnTransformer\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.model_selection import train_test_split, StratifiedKFold, cross_val_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.metrics import roc_auc_score, classification_report, confusion_matrix\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Load the data\n",
    "def load_data(file_path):\n",
    "    \"\"\"Load customer churn data\"\"\"\n",
    "    df = pd.read_csv(file_path)\n",
    "    print(f\"Dataset shape: {df.shape}\")\n",
    "    return df\n",
    "\n",
    "# 1. Basic Preprocessing\n",
    "def preprocess_data(df):\n",
    "    \"\"\"Basic preprocessing steps\"\"\"\n",
    "    # Create a copy to avoid modifying the original dataframe\n",
    "    data = df.copy()\n",
    "    \n",
    "    # Drop customer ID as it's not predictive\n",
    "    if 'CustomerID' in data.columns:\n",
    "        data.drop('CustomerID', axis=1, inplace=True)\n",
    "    \n",
    "    # Convert 'TotalCharges' to numeric if it's not already\n",
    "    if data['TotalCharges'].dtype == 'object':\n",
    "        data['TotalCharges'] = pd.to_numeric(data['TotalCharges'], errors='coerce')\n",
    "        # Fill missing values with 0 or median\n",
    "        data['TotalCharges'].fillna(0, inplace=True)\n",
    "    \n",
    "    # Convert binary categorical values to numeric (0/1)\n",
    "    binary_cols = ['SeniorCitizen']\n",
    "    for col in ['Gender', 'Partner', 'Dependents', 'PhoneService', 'Churn']:\n",
    "        if col in data.columns:\n",
    "            data[col] = data[col].map({'Yes': 1, 'No': 0, 'Male': 1, 'Female': 0})\n",
    "            binary_cols.append(col)\n",
    "    \n",
    "    # Create X (features) and y (target)\n",
    "    y = data['Churn']\n",
    "    X = data.drop('Churn', axis=1)\n",
    "    \n",
    "    return X, y, binary_cols\n",
    "\n",
    "# 2. Feature Engineering Function\n",
    "def engineer_features(X, y=None):\n",
    "    \"\"\"Create new features to improve model performance\"\"\"\n",
    "    data = X.copy()\n",
    "    \n",
    "    # A. Customer Longevity Features\n",
    "    \n",
    "    # Tenure bins\n",
    "    data['TenureBin'] = pd.cut(data['Tenure'], \n",
    "                              bins=[0, 12, 24, 36, 48, 60, float('inf')],\n",
    "                              labels=[1, 2, 3, 4, 5, 6])\n",
    "    data['IsNewCustomer'] = (data['Tenure'] <= 6).astype(int)\n",
    "    data['IsLongTermCustomer'] = (data['Tenure'] >= 36).astype(int)\n",
    "    \n",
    "    # Average Monthly Spend\n",
    "    data['AvgMonthlySpend'] = data['TotalCharges'] / (data['Tenure'] + 1)  # Add 1 to avoid division by zero\n",
    "    \n",
    "    # Spending pattern (is their monthly charge higher than their average?)\n",
    "    data['SpendingPattern'] = (data['MonthlyCharges'] > data['AvgMonthlySpend']).astype(int)\n",
    "    \n",
    "    # B. Financial Features\n",
    "    \n",
    "    # Log transformation of charges (for Logistic Regression)\n",
    "    data['LogMonthlyCharges'] = np.log1p(data['MonthlyCharges'])\n",
    "    data['LogTotalCharges'] = np.log1p(data['TotalCharges'])\n",
    "    \n",
    "    # Price sensitivity flags\n",
    "    month_charge_quantiles = data['MonthlyCharges'].quantile([0.25, 0.75]).values\n",
    "    data['IsLowSpender'] = (data['MonthlyCharges'] <= month_charge_quantiles[0]).astype(int)\n",
    "    data['IsHighSpender'] = (data['MonthlyCharges'] >= month_charge_quantiles[1]).astype(int)\n",
    "    \n",
    "    # C. Service Features\n",
    "    \n",
    "    # Create dummies for InternetService and Contract\n",
    "    # These will be handled by the column transformer, but we'll create some combinations\n",
    "    \n",
    "    # Create service complexity score (number of services)\n",
    "    # Assuming 'Yes' = 1 and 'No' = 0 for service columns after preprocessing\n",
    "    service_cols = ['PhoneService']\n",
    "    if 'InternetService' in data.columns:\n",
    "        # For InternetService, create binary indicators\n",
    "        data['HasFiberOptic'] = (data['InternetService'] == 'Fiber optic').astype(int)\n",
    "        data['HasDSL'] = (data['InternetService'] == 'DSL').astype(int)\n",
    "        data['HasNoInternet'] = (data['InternetService'] == 'No').astype(int)\n",
    "        service_cols.extend(['HasFiberOptic', 'HasDSL'])\n",
    "    \n",
    "    # Contract Type\n",
    "    if 'Contract' in data.columns:\n",
    "        data['IsMonthToMonth'] = (data['Contract'] == 'Month-to-month').astype(int)\n",
    "        data['IsOneYear'] = (data['Contract'] == 'One year').astype(int)\n",
    "        data['IsTwoYear'] = (data['Contract'] == 'Two year').astype(int)\n",
    "        \n",
    "        # After creating the binary features, we can drop the original categorical columns\n",
    "        # to avoid duplication (this helps with some machine learning algorithms)\n",
    "        data.drop(['InternetService', 'Contract'], axis=1, inplace=True)\n",
    "    \n",
    "    # D. Interaction Features\n",
    "    \n",
    "    # Customer demographics + financial\n",
    "    if 'SeniorCitizen' in data.columns:\n",
    "        data['Senior_HighSpender'] = data['SeniorCitizen'] * data['IsHighSpender']\n",
    "    \n",
    "    if 'Partner' in data.columns:\n",
    "        data['Partner_LongTerm'] = data['Partner'] * data['IsLongTermCustomer']\n",
    "    \n",
    "    # Service + Contract interaction\n",
    "    if 'HasFiberOptic' in data.columns and 'IsMonthToMonth' in data.columns:\n",
    "        data['FiberOptic_MonthToMonth'] = data['HasFiberOptic'] * data['IsMonthToMonth']\n",
    "        # This is a high churn-risk group typically\n",
    "    \n",
    "    # E. Non-linear transformations (especially for Logistic Regression)\n",
    "    \n",
    "    # Polynomial features for Tenure\n",
    "    data['Tenure_Squared'] = data['Tenure'] ** 2\n",
    "    \n",
    "    # Ratio features\n",
    "    data['Charges_Tenure_Ratio'] = data['TotalCharges'] / (data['Tenure'] + 1)\n",
    "    \n",
    "    # F. Additional domain-specific features\n",
    "    \n",
    "    # Loyalty Index (higher for customers with long tenure and longer contracts)\n",
    "    if 'IsOneYear' in data.columns and 'IsTwoYear' in data.columns:\n",
    "        contract_score = data['IsMonthToMonth'] * 1 + data['IsOneYear'] * 2 + data['IsTwoYear'] * 3\n",
    "        data['LoyaltyIndex'] = (data['Tenure'] / 72) * 0.5 + (contract_score / 3) * 0.5\n",
    "    \n",
    "    return data\n",
    "\n",
    "# 3. Build preprocessing pipelines specific to each model\n",
    "def build_preprocessors(X, binary_cols):\n",
    "    \"\"\"Build preprocessing pipelines for Random Forest and Logistic Regression\"\"\"\n",
    "    \n",
    "    # Identify column types\n",
    "    categorical_cols = [col for col in X.columns if X[col].dtype == 'object']\n",
    "    numerical_cols = [col for col in X.columns if X[col].dtype in ['int64', 'float64'] and col not in binary_cols]\n",
    "    \n",
    "    print(f\"Categorical columns: {categorical_cols}\")\n",
    "    print(f\"Numerical columns: {numerical_cols}\")\n",
    "    print(f\"Binary columns: {binary_cols}\")\n",
    "    \n",
    "    # Create preprocessing pipelines using list of column indices rather than names\n",
    "    # to avoid the KeyError in cross-validation\n",
    "    rf_transformers = []\n",
    "    lr_transformers = []\n",
    "    \n",
    "    # Add numerical features\n",
    "    if numerical_cols:\n",
    "        rf_transformers.append(('num', 'passthrough', numerical_cols))\n",
    "        lr_transformers.append(('num', StandardScaler(), numerical_cols))\n",
    "        \n",
    "    # Add categorical features\n",
    "    if categorical_cols:\n",
    "        rf_transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols))\n",
    "        lr_transformers.append(('cat', OneHotEncoder(handle_unknown='ignore'), categorical_cols))\n",
    "    \n",
    "    # Add binary features\n",
    "    if binary_cols:\n",
    "        rf_transformers.append(('bin', 'passthrough', binary_cols))\n",
    "        lr_transformers.append(('bin', 'passthrough', binary_cols))\n",
    "    \n",
    "    # For Random Forest\n",
    "    rf_preprocessor = ColumnTransformer(\n",
    "        transformers=rf_transformers,\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    # For Logistic Regression\n",
    "    lr_preprocessor = ColumnTransformer(\n",
    "        transformers=lr_transformers,\n",
    "        remainder='drop'\n",
    "    )\n",
    "    \n",
    "    return rf_preprocessor, lr_preprocessor\n",
    "\n",
    "# 4. Model Training and Evaluation\n",
    "def train_and_evaluate_models(X, y, rf_preprocessor, lr_preprocessor):\n",
    "    \"\"\"Train and evaluate Random Forest and Logistic Regression models\"\"\"\n",
    "    \n",
    "    # Split data\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.25, random_state=42, stratify=y)\n",
    "    \n",
    "    # Define models\n",
    "    rf_model = RandomForestClassifier(n_estimators=100, random_state=42, class_weight='balanced')\n",
    "    lr_model = LogisticRegression(max_iter=1000, random_state=42, class_weight='balanced')\n",
    "    \n",
    "    # Create pipelines\n",
    "    rf_pipeline = Pipeline([\n",
    "        ('preprocessor', rf_preprocessor),\n",
    "        ('classifier', rf_model)\n",
    "    ])\n",
    "    \n",
    "    lr_pipeline = Pipeline([\n",
    "        ('preprocessor', lr_preprocessor),\n",
    "        ('classifier', lr_model)\n",
    "    ])\n",
    "    \n",
    "    try:\n",
    "        # Train models directly (skipping cross-validation for now to debug)\n",
    "        print(\"\\nTraining Random Forest model...\")\n",
    "        rf_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        print(\"Training Logistic Regression model...\")\n",
    "        lr_pipeline.fit(X_train, y_train)\n",
    "        \n",
    "        # Evaluate on test set\n",
    "        rf_pred_proba = rf_pipeline.predict_proba(X_test)[:, 1]\n",
    "        lr_pred_proba = lr_pipeline.predict_proba(X_test)[:, 1]\n",
    "        \n",
    "        rf_auc = roc_auc_score(y_test, rf_pred_proba)\n",
    "        lr_auc = roc_auc_score(y_test, lr_pred_proba)\n",
    "        \n",
    "        print(\"\\nTest Set Results:\")\n",
    "        print(f\"Random Forest ROC AUC: {rf_auc:.4f}\")\n",
    "        print(f\"Logistic Regression ROC AUC: {lr_auc:.4f}\")\n",
    "        \n",
    "        # Generate confusion matrix and classification report for RF\n",
    "        rf_pred = rf_pipeline.predict(X_test)\n",
    "        print(\"\\nRandom Forest Confusion Matrix:\")\n",
    "        print(confusion_matrix(y_test, rf_pred))\n",
    "        print(\"\\nRandom Forest Classification Report:\")\n",
    "        print(classification_report(y_test, rf_pred))\n",
    "        \n",
    "        # Once the basic models work, we can try cross-validation\n",
    "        # Cross-validation\n",
    "        cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "        \n",
    "        print(\"\\nPerforming Random Forest Cross-Validation:\")\n",
    "        try:\n",
    "            rf_scores = cross_val_score(rf_pipeline, X_train, y_train, cv=cv, scoring='roc_auc', error_score='raise')\n",
    "            print(f\"RF ROC AUC: {rf_scores.mean():.4f} (±{rf_scores.std():.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Random Forest cross-validation error: {e}\")\n",
    "        \n",
    "        print(\"\\nPerforming Logistic Regression Cross-Validation:\")\n",
    "        try:\n",
    "            lr_scores = cross_val_score(lr_pipeline, X_train, y_train, cv=cv, scoring='roc_auc', error_score='raise')\n",
    "            print(f\"LR ROC AUC: {lr_scores.mean():.4f} (±{lr_scores.std():.4f})\")\n",
    "        except Exception as e:\n",
    "            print(f\"Logistic Regression cross-validation error: {e}\")\n",
    "        \n",
    "        # Feature importance for Random Forest (with additional error handling)\n",
    "        try:\n",
    "            if hasattr(rf_pipeline.named_steps['classifier'], 'feature_importances_'):\n",
    "                # Get feature importances\n",
    "                importances = rf_pipeline.named_steps['classifier'].feature_importances_\n",
    "                \n",
    "                # Get the names of features after preprocessing\n",
    "                # This is a simplified approach that may not perfectly map feature names\n",
    "                # but will at least provide some insight\n",
    "                print(\"\\nTop feature importances (Random Forest):\")\n",
    "                feature_importance = pd.DataFrame({\n",
    "                    'importance': importances\n",
    "                })\n",
    "                feature_importance = feature_importance.sort_values('importance', ascending=False)\n",
    "                print(feature_importance.head(15))\n",
    "        except Exception as e:\n",
    "            print(f\"Error getting feature importances: {e}\")\n",
    "            \n",
    "    except Exception as e:\n",
    "        print(f\"Error during model training: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "        \n",
    "    return rf_pipeline, lr_pipeline\n",
    "\n",
    "# 5. Main function\n",
    "def main():\n",
    "    \"\"\"Main execution function\"\"\"\n",
    "    \n",
    "    try:\n",
    "        # 1. Load data - Allow passing file path as parameter\n",
    "        import sys\n",
    "        file_path = 'customer_churn.csv'\n",
    "        if len(sys.argv) > 1:\n",
    "            file_path = sys.argv[1]\n",
    "        \n",
    "        print(f\"Loading data from: {file_path}\")\n",
    "        df = load_data(file_path)\n",
    "        \n",
    "        # 2. Basic preprocessing\n",
    "        X, y, binary_cols = preprocess_data(df)\n",
    "        print(f\"Data shape after preprocessing: {X.shape}\")\n",
    "        \n",
    "        # 3. Engineer features\n",
    "        X_engineered = engineer_features(X, y)\n",
    "        print(f\"Shape after feature engineering: {X_engineered.shape}\")\n",
    "        \n",
    "        # 4. Build preprocessors\n",
    "        rf_preprocessor, lr_preprocessor = build_preprocessors(X_engineered, binary_cols)\n",
    "        \n",
    "        # 5. Train and evaluate models\n",
    "        rf_model, lr_model = train_and_evaluate_models(X_engineered, y, rf_preprocessor, lr_preprocessor)\n",
    "        \n",
    "        # 6. Save models\n",
    "        from joblib import dump\n",
    "        dump(rf_model, 'random_forest_model.joblib')\n",
    "        dump(lr_model, 'logistic_regression_model.joblib')\n",
    "        \n",
    "        print(\"\\nFeature engineering pipeline completed successfully!\")\n",
    "        print(\"Models saved as 'random_forest_model.joblib' and 'logistic_regression_model.joblib'\")\n",
    "        \n",
    "    except Exception as e:\n",
    "        print(f\"Error in main execution: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "    \n",
    "main()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "venv"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
